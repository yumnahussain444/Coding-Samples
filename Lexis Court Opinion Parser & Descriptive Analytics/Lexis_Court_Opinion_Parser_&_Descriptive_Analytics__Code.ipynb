{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFImL-vIe9CJ"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Yumna Hussain - Lexis Court Opinion Parser & Descriptive Analytics\n",
        "# ------------------------------------------------------------\n",
        "# - Installs libraries quietly\n",
        "# - Mounts Google Drive\n",
        "# - Points to input TXT directory and output folder\n",
        "# ============================================================\n",
        "\n",
        "%pip install -q pandas pyarrow charset-normalizer python-dateutil tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "from pathlib import Path\n",
        "from itertools import islice\n",
        "\n",
        "TXT_DIR = Path(\"/content/drive/MyDrive/Stigler Center DT/case_text/case_text\")\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/Stigler Center DT/out\")\n",
        "\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "txts = list(TXT_DIR.rglob(\"*.txt\"))\n",
        "print(\"Found\", len(txts), \"txt files\")\n",
        "for p in islice(txts, 5):\n",
        "    print(\" -\", p)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Lexis parser\n",
        "# ------------------------------------------------------------\n",
        "# - Handles non-strict JSON-like text (arrays with ']' inside quotes)\n",
        "# - Extracts core variables used downstream\n",
        "# - Lightly normalizes court names and derives jurisdiction/level/state\n",
        "# - Builds best-available opinion text\n",
        "# ============================================================\n",
        "\n",
        "import re, csv, sys\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from dateutil import parser as dateparser\n",
        "from charset_normalizer import from_path\n",
        "\n",
        "def read_text(p: Path) -> str:\n",
        "    # tolerant reader for varied encodings\n",
        "    try:\n",
        "        return p.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
        "    except Exception:\n",
        "        best = from_path(str(p)).best()\n",
        "        return str(best) if best is not None else p.read_bytes().decode(\"latin-1\", errors=\"replace\")\n",
        "\n",
        "def _extract_array_strings(text: str, key: str):\n",
        "    # pull strings from `\"key\": [ \"…\", \"…\" ]` while honoring quotes/escapes\n",
        "    m = re.search(rf'\"{re.escape(key)}\"\\s*:', text, flags=re.I)\n",
        "    if not m: return []\n",
        "    pos = m.end()\n",
        "    i = text.find('[', pos)\n",
        "    if i == -1: return []\n",
        "    i += 1\n",
        "    depth, in_str, esc = 1, False, False\n",
        "    buf = []\n",
        "    for ch in text[i:]:\n",
        "        buf.append(ch)\n",
        "        if in_str:\n",
        "            if esc: esc = False\n",
        "            elif ch == '\\\\': esc = True\n",
        "            elif ch == '\"': in_str = False\n",
        "        else:\n",
        "            if ch == '\"': in_str = True\n",
        "            elif ch == '[': depth += 1\n",
        "            elif ch == ']':\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    break\n",
        "    segment = ''.join(buf)\n",
        "    vals, cur, in_str, esc = [], \"\", False, False\n",
        "    for ch in segment:\n",
        "        if in_str:\n",
        "            if esc:\n",
        "                cur += ch; esc = False\n",
        "            elif ch == '\\\\':\n",
        "                esc = True\n",
        "            elif ch == '\"':\n",
        "                in_str = False; vals.append(cur); cur = \"\"\n",
        "            else:\n",
        "                cur += ch\n",
        "        else:\n",
        "            if ch == '\"': in_str = True\n",
        "            elif ch == ']': break\n",
        "    return [v.strip() for v in vals if v is not None]\n",
        "\n",
        "def grab_list(text: str, key: str):\n",
        "    vals = _extract_array_strings(text, key)\n",
        "    return vals if vals else []\n",
        "\n",
        "def grab_one(text: str, key: str):\n",
        "    # array first, then scalar `\"key\": \"value\"`\n",
        "    vals = grab_list(text, key)\n",
        "    if vals: return vals[0]\n",
        "    m = re.search(rf'\"{re.escape(key)}\"\\s*:\\s*\"([^\"]+)\"', text, flags=re.I|re.S)\n",
        "    return m.group(1).strip() if m else None\n",
        "\n",
        "def smart_space_fix(s: str|None) -> str|None:\n",
        "    if not s: return s\n",
        "    s = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', s)\n",
        "    s = (s.replace(\"UnitedStates\", \"United States\")\n",
        "           .replace(\"Court ofAppeals\", \"Court of Appeals\")\n",
        "           .replace(\"Appealsfor\", \"Appeals for\")\n",
        "           .replace(\"DistrictCourt\", \"District Court\")\n",
        "           .replace(\"Courtfor\", \"Court for\"))\n",
        "    return re.sub(r'\\s+', ' ', s).strip()\n",
        "\n",
        "STATE_NAMES = {\n",
        "    \"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\n",
        "    \"Hawaii\",\"Idaho\",\"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
        "    \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\"Nevada\",\n",
        "    \"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\n",
        "    \"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
        "    \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\",\"District of Columbia\"\n",
        "}\n",
        "\n",
        "def parse_decision_date(text: str):\n",
        "    # tries several keys; returns (ISO date or None, year or None)\n",
        "    for key in (\"decisionDate\",\"filedDate\",\"dateText\",\"decisionDates\",\"filedDates\"):\n",
        "        v = grab_one(text, key)\n",
        "        if v:\n",
        "            head = \", \".join(v.split(\",\")[:2]).strip()\n",
        "            try:\n",
        "                dt = dateparser.parse(head, fuzzy=True)\n",
        "                return dt.date().isoformat(), dt.year\n",
        "            except Exception:\n",
        "                pass\n",
        "    m = re.search(r'\\b(19|20)\\d{2}\\b', text)\n",
        "    return (None, int(m.group(0))) if m else (None, None)\n",
        "\n",
        "def extract_opinion_text(raw: str) -> str:\n",
        "    # picks the most structured field; falls back to scanning for \"OPINION\"\n",
        "    for key in (\"courtCaseDocBody\",\"caseOpinions\",\"opinion\",\"opinionText\",\"bodyText\",\"p\"):\n",
        "        arr = grab_list(raw, key)\n",
        "        if arr: return \"\\n\\n\".join(arr).strip()\n",
        "        one = grab_one(raw, key)\n",
        "        if one: return one.strip()\n",
        "    m = re.search(r'(?:^|\\n)\\s*(OPINION|Per Curiam|Opinion)\\b(.*)$', raw, flags=re.I|re.S)\n",
        "    return m.group(2).strip() if m else \"\"\n",
        "\n",
        "OFFICIAL_REPORTER_RE = re.compile(\n",
        "    r'\\b(U\\.S\\.|S\\.\\s*Ct\\.|L\\.\\s*Ed\\.|F\\.\\d+d?|F\\.Supp\\.|F\\.App\\'?x|P\\.\\d+d?|N\\.[EW]\\.\\d+d?|So\\.\\d+d?|Cal\\.\\s*App\\.|A\\.\\d+d?)\\b'\n",
        ")\n",
        "def infer_publication_status(citations: str|None) -> str|None:\n",
        "    if not citations: return None\n",
        "    if OFFICIAL_REPORTER_RE.search(citations): return \"Published\"\n",
        "    if re.search(r'\\b(LEXIS|WL)\\b', citations): return \"Unpublished\"\n",
        "    return None\n",
        "\n",
        "def derive_jurisdiction(court_name: str|None) -> str|None:\n",
        "    if not court_name: return None\n",
        "    s = court_name.lower()\n",
        "    if \"united states\" in s or \"u.s.\" in s or \"federal\" in s: return \"federal\"\n",
        "    return \"state\"\n",
        "\n",
        "def derive_court_level(court_name: str|None, jurisdiction: str|None) -> str|None:\n",
        "    if not court_name: return None\n",
        "    s = court_name.lower()\n",
        "    if jurisdiction == \"federal\":\n",
        "        if \"supreme court\" in s: return \"federal_supreme\"\n",
        "        if \"court of appeals\" in s or \"circuit\" in s: return \"federal_appellate\"\n",
        "        if \"district court\" in s: return \"federal_district\"\n",
        "    if \"supreme judicial court\" in s or \"supreme court\" in s: return \"state_supreme\"\n",
        "    if \"court of criminal appeals\" in s or \"court of appeals\" in s or \"appellate\" in s: return \"intermediate_appellate\"\n",
        "    if \"district court\" in s or \"superior court\" in s or \"trial court\" in s: return \"trial\"\n",
        "    return None\n",
        "\n",
        "def derive_state(court_name: str|None, jurisdiction: str|None) -> str|None:\n",
        "    if not court_name or jurisdiction == \"federal\": return None\n",
        "    m = re.search(r'\\bof\\s+([A-Za-z\\s\\.]+)$', court_name)\n",
        "    if m:\n",
        "        tail = m.group(1).strip().rstrip('.')\n",
        "        for st in sorted(STATE_NAMES, key=len, reverse=True):\n",
        "            if re.search(rf'\\b{re.escape(st)}\\b', tail): return st\n",
        "        return tail\n",
        "    return None\n",
        "\n",
        "def split_citations(text: str) -> str|None:\n",
        "    cites = grab_list(text, \"citeForThisResource\")\n",
        "    if cites:\n",
        "        seen, out = set(), []\n",
        "        for c in cites:\n",
        "            if c not in seen:\n",
        "                out.append(c); seen.add(c)\n",
        "        return \"; \".join(out)\n",
        "    c = grab_one(text, \"citations\")\n",
        "    if c:\n",
        "        parts = re.split(r'(?=(?:\\b[A-Z][\\w\\.]*\\s+\\d+\\b)|(?:\\b\\d{4}\\s+(?:WL|LEXIS)\\b))', c)\n",
        "        parts = [p.strip() for p in parts if p.strip()]\n",
        "        return \"; \".join(dict.fromkeys(parts))\n",
        "    return None\n",
        "\n",
        "def safe_short_case(case_name: str|None) -> str|None:\n",
        "    if not case_name: return None\n",
        "    m = re.search(r'((?:In re|In the matter of)[^,\\n]+)', case_name, flags=re.I)\n",
        "    if m: return m.group(1).strip()\n",
        "    m = re.search(r'([^,\\n]*?\\s+v\\.\\s+[^,\\n]+)', case_name)\n",
        "    return m.group(1).strip() if m else case_name\n",
        "\n",
        "def parse_file(p: Path) -> dict:\n",
        "    raw = read_text(p)\n",
        "    case_name  = grab_one(raw, \"fullCaseName\") or grab_one(raw, \"caseName\")\n",
        "    short_case = grab_one(raw, \"shortCaseName\") or safe_short_case(case_name)\n",
        "    docket     = grab_one(raw, \"docketNumber\")\n",
        "    court_name = smart_space_fix(grab_one(raw, \"courtName\") or grab_one(raw, \"courtInfo\"))\n",
        "    decided_date, year = parse_decision_date(raw)\n",
        "    citations = split_citations(raw)\n",
        "    publication_status = grab_one(raw, \"publicationStatus\") or grab_one(raw, \"statusNotice\") or infer_publication_status(citations)\n",
        "\n",
        "    # disposition (simple pattern search)\n",
        "    dispo_patterns = [\n",
        "        r'\\bAffirmed\\.?', r'\\bReversed\\.?', r'\\bVacated\\.?', r'\\bRemanded\\.?',\n",
        "        r'\\bDenied\\.?', r'\\bDismissed\\.?', r'\\bPetition for review denied\\.?',\n",
        "        r'\\bJudgment (?:is )?affirmed\\.?', r'\\bJudgment (?:is )?reversed\\.?'\n",
        "    ]\n",
        "    dispo_re = re.compile(\"|\".join(dispo_patterns), flags=re.I)\n",
        "    joined = \" \".join(grab_list(raw, \"p\") + grab_list(raw, \"courtCaseDocBody\") + grab_list(raw, \"bodyText\"))\n",
        "    m = dispo_re.search(joined or raw)\n",
        "    disposition = m.group(0).rstrip('.').strip().capitalize() if m else None\n",
        "\n",
        "    jurisdiction = derive_jurisdiction(court_name)\n",
        "    court_level  = derive_court_level(court_name, jurisdiction)\n",
        "    state        = derive_state(court_name, jurisdiction)\n",
        "\n",
        "    if not docket:\n",
        "        m = re.search(r'\\bS\\d{5,6}\\b', raw) or re.search(r'\\b[A-Z]{1,3}-\\d{2}-\\d{4,6}\\b', raw) \\\n",
        "            or re.search(r'\\b\\d{2,4}[A-Z]{0,3}\\d{2,6}-?[A-Z]{0,3}\\b', raw)\n",
        "        docket = m.group(0) if m else None\n",
        "\n",
        "    text = extract_opinion_text(raw)\n",
        "\n",
        "    return {\n",
        "        \"doc_id\": p.stem,\n",
        "        \"case_name\": case_name,\n",
        "        \"short_case_name\": short_case,\n",
        "        \"docket_number\": docket,\n",
        "        \"court_name\": court_name,\n",
        "        \"jurisdiction\": jurisdiction,\n",
        "        \"court_level\": court_level,\n",
        "        \"state\": state,\n",
        "        \"decided_date\": decided_date,\n",
        "        \"year\": year,\n",
        "        \"citations\": citations,\n",
        "        \"publication_status\": publication_status,\n",
        "        \"disposition\": disposition,\n",
        "        \"text\": text\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "qoZllN9cfBzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Parse all TXT files, tidy DataFrame, save TSV/CSV/Parquet\n",
        "# ------------------------------------------------------------\n",
        "# - Iterates through TXT_DIR\n",
        "# - Catches errors but preserves row with _error note\n",
        "# - Quick QC print for suspicious rows\n",
        "# - Writes cases_clean.(tsv|csv|parquet) to OUT_DIR\n",
        "# ============================================================\n",
        "\n",
        "paths = sorted(Path(TXT_DIR).rglob(\"*.txt\"))\n",
        "rows = []\n",
        "for p in tqdm(paths, desc=\"Parsing\"):\n",
        "    try:\n",
        "        rows.append(parse_file(p))\n",
        "    except Exception as e:\n",
        "        rows.append({\n",
        "            \"doc_id\": p.stem, \"case_name\": None, \"short_case_name\": None, \"docket_number\": None,\n",
        "            \"court_name\": None, \"jurisdiction\": None, \"court_level\": None, \"state\": None,\n",
        "            \"decided_date\": None, \"year\": None, \"citations\": None, \"publication_status\": None,\n",
        "            \"disposition\": None, \"text\": \"\", \"_error\": str(e)\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "bad_mask = (df[\"text\"].fillna(\"\") == \"\") | (df[\"text\"].str.startswith(\"{\", na=False))\n",
        "print(\"Potentially bad rows:\", bad_mask.sum())\n",
        "print(df.loc[bad_mask, [\"doc_id\",\"court_name\",\"decided_date\",\"citations\"]].head(10))\n",
        "\n",
        "cols = [\"doc_id\",\"case_name\",\"short_case_name\",\"docket_number\",\"court_name\",\"jurisdiction\",\n",
        "        \"court_level\",\"state\",\"decided_date\",\"year\",\"citations\",\"publication_status\",\n",
        "        \"disposition\",\"text\"]\n",
        "for c in cols:\n",
        "    if c not in df.columns:\n",
        "        df[c] = None\n",
        "df = df[cols + [c for c in df.columns if c not in cols]]\n",
        "\n",
        "tsv_path  = OUT_DIR / \"cases_clean.tsv\"\n",
        "csv_path  = OUT_DIR / \"cases_clean.csv\"\n",
        "parq_path = OUT_DIR / \"cases_clean.parquet\"\n",
        "\n",
        "df.to_csv(tsv_path, sep=\"\\t\", index=False)\n",
        "df.to_csv(csv_path, index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "try:\n",
        "    df.to_parquet(parq_path, index=False)\n",
        "except Exception as e:\n",
        "    print(\"Parquet write skipped:\", e)\n",
        "\n",
        "print(\"Wrote:\\n -\", tsv_path, \"\\n -\", csv_path, \"\\n -\", parq_path if parq_path.exists() else \"(parquet skipped)\")\n"
      ],
      "metadata": {
        "id": "Hj-PwEkcE40E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# (Q1): Case distribution over time\n",
        "# ------------------------------------------------------------\n",
        "# - Builds yearly counts (and 3-year MA + share) from cases_clean\n",
        "# - Saves summary table to OUT_DIR/T1_cases_by_year.csv\n",
        "# - Saves figure PNG/PDF to OUT_DIR/figures\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd, re\n",
        "from dateutil import parser as dateparser\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "FIG_DIR = (OUT_DIR / \"figures\"); FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "by_year_path = OUT_DIR / \"T1_cases_by_year.csv\"\n",
        "\n",
        "# load cases\n",
        "tsv = OUT_DIR / \"cases_clean.tsv\"\n",
        "csv = OUT_DIR / \"cases_clean.csv\"\n",
        "if tsv.exists():\n",
        "    df_cases = pd.read_csv(tsv, sep=\"\\t\", dtype=str, keep_default_na=False)\n",
        "else:\n",
        "    df_cases = pd.read_csv(csv, dtype=str, keep_default_na=False)\n",
        "\n",
        "# coerce year (explicit year > decided_date > scan text)\n",
        "def coerce_year(row):\n",
        "    y = row.get(\"year\")\n",
        "    if isinstance(y, str) and y.strip().isdigit():\n",
        "        return int(y)\n",
        "    d = row.get(\"decided_date\")\n",
        "    if isinstance(d, str) and d.strip():\n",
        "        try:\n",
        "            return dateparser.parse(d, fuzzy=True).year\n",
        "        except Exception:\n",
        "            pass\n",
        "    txt = row.get(\"text\") or \"\"\n",
        "    m = re.search(r'\\b(19|20)\\d{2}\\b', txt)\n",
        "    return int(m.group(0)) if m else None\n",
        "\n",
        "df_cases[\"_year_coerced\"] = df_cases.apply(coerce_year, axis=1)\n",
        "by_year = (df_cases.dropna(subset=[\"_year_coerced\"])\n",
        "            .assign(_year_coerced=lambda x: x[\"_year_coerced\"].astype(int))\n",
        "            .groupby(\"_year_coerced\").size().reset_index(name=\"n_cases\")\n",
        "            .sort_values(\"_year_coerced\"))\n",
        "\n",
        "total = by_year[\"n_cases\"].sum()\n",
        "by_year[\"share_of_total\"] = (by_year[\"n_cases\"] / total).round(4)\n",
        "by_year[\"n_cases_3yr_ma\"] = by_year[\"n_cases\"].rolling(window=3, min_periods=1).mean().round(1)\n",
        "\n",
        "by_year.to_csv(by_year_path, index=False)\n",
        "print(\"Saved:\", by_year_path)\n",
        "\n",
        "# line chart (no explicit colors per requirement)\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax.plot(by_year[\"_year_coerced\"], by_year[\"n_cases\"])\n",
        "ax.set_title(\"Case Distribution by Year\")\n",
        "ax.set_xlabel(\"Year\")\n",
        "ax.set_ylabel(\"Number of cases\")\n",
        "fig.tight_layout()\n",
        "\n",
        "png_path = FIG_DIR / \"cases_by_year.png\"\n",
        "pdf_path = FIG_DIR / \"cases_by_year.pdf\"\n",
        "fig.savefig(png_path, dpi=300, bbox_inches=\"tight\")\n",
        "fig.savefig(pdf_path, bbox_inches=\"tight\")\n",
        "plt.close(fig)\n",
        "print(\"Saved figure:\\n -\", png_path, \"\\n -\", pdf_path)\n"
      ],
      "metadata": {
        "id": "B68bG5t2E_j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# (Q2): Top-5 judges — Supreme Court of Florida\n",
        "# ------------------------------------------------------------\n",
        "# - Filters to Florida Supreme Court\n",
        "# - Cleans judge tokens (drops titles like CJ/J./Chief Justice)\n",
        "# - Merges small variants by last name (canonical key)\n",
        "# - Counts unique cases per judge and saves table\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd, numpy as np, re\n",
        "\n",
        "READ_TSV = OUT_DIR / \"cases_clean.tsv\"\n",
        "READ_CSV = OUT_DIR / \"cases_clean.csv\"\n",
        "fl_df = pd.read_csv(READ_TSV, sep=\"\\t\", dtype=str) if READ_TSV.exists() else pd.read_csv(READ_CSV, dtype=str)\n",
        "\n",
        "def is_fl_sct(cname: str|None) -> bool:\n",
        "    if not isinstance(cname, str): return False\n",
        "    s = cname.lower().strip()\n",
        "    return (\"supreme court of florida\" in s) or (s == \"florida supreme court\")\n",
        "\n",
        "fl = fl_df[fl_df[\"court_name\"].map(is_fl_sct)].copy()\n",
        "fl[\"judges\"] = fl.get(\"judges\", pd.Series(index=fl.index, dtype=\"object\")).fillna(\"\")\n",
        "\n",
        "TITLE_TOKENS = re.compile(\n",
        "    r'\\b(?:C\\.?J\\.?|P\\.?J\\.?|J\\.|JJ\\.|Chief Justice|Presiding Justice|Justice|Acting|Temporarily Assigned|For the Court)\\b',\n",
        "    re.I\n",
        ")\n",
        "DROP_SET = {\"\", \"cj\", \"j\", \"jj\", \"chief justice\", \"justice\", \"per curiam\", \"the court\"}\n",
        "SUFFIXES = {\"Jr\",\"Jr.\",\"Sr\",\"Sr.\",\"II\",\"III\",\"IV\"}\n",
        "\n",
        "def clean_token(tok: str) -> str|None:\n",
        "    if not isinstance(tok, str): return None\n",
        "    s = tok.strip()\n",
        "    if not s: return None\n",
        "    s = re.sub(r'\\([^)]*\\)', '', s)                 # remove (dissenting), etc.\n",
        "    s = TITLE_TOKENS.sub('', s)                     # drop titles\n",
        "    s = re.sub(r'^(?:and|the|hon\\.?|honorable)\\s+', '', s, flags=re.I)\n",
        "    s = re.sub(r'[;:]+', ' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip(' ,;:')\n",
        "    low = s.lower()\n",
        "    if low in DROP_SET or 'per curiam' in low: return None\n",
        "    if len(re.sub(r'[^A-Za-z]', '', s)) < 2: return None\n",
        "    if ',' in s and not re.search(r'\\b(?:Jr\\.?|Sr\\.?|II|III|IV)\\b$', s):\n",
        "        left, right = [p.strip() for p in s.split(',', 1)]\n",
        "        if len(left.split()) == 1 and len(right) >= 2:\n",
        "            s = (right + ' ' + left).strip()\n",
        "    if s.isupper(): s = s.title()\n",
        "    return s\n",
        "\n",
        "def canonical_last(name: str) -> str|None:\n",
        "    if not name: return None\n",
        "    parts = [p for p in name.split() if p not in SUFFIXES]\n",
        "    if not parts: return None\n",
        "    return parts[-1].upper()\n",
        "\n",
        "def split_judges(val: str):\n",
        "    if not isinstance(val, str) or not val.strip(): return []\n",
        "    return [j.strip() for j in val.split(\"|\") if j.strip()]\n",
        "\n",
        "rows = []\n",
        "for _, r in fl.iterrows():\n",
        "    for j in split_judges(r[\"judges\"]):\n",
        "        cj = clean_token(j)\n",
        "        if cj:\n",
        "            rows.append({\"judge_full\": cj, \"judge_key\": canonical_last(cj), \"doc_id\": r[\"doc_id\"]})\n",
        "panel_df = pd.DataFrame(rows)\n",
        "\n",
        "if panel_df.empty:\n",
        "    print(\"No judge names found in FL Supreme Court entries.\")\n",
        "else:\n",
        "    name_choice = (panel_df.groupby([\"judge_key\",\"judge_full\"]).size()\n",
        "                     .reset_index(name=\"n\")\n",
        "                     .sort_values([\"judge_key\",\"n\"], ascending=[True, False])\n",
        "                     .drop_duplicates(\"judge_key\")[[\"judge_key\",\"judge_full\"]])\n",
        "\n",
        "    counts = (panel_df.drop_duplicates([\"judge_key\",\"doc_id\"])\n",
        "                        .groupby(\"judge_key\").size()\n",
        "                        .reset_index(name=\"n_cases\"))\n",
        "\n",
        "    out = (counts.merge(name_choice, on=\"judge_key\", how=\"left\")\n",
        "                 .rename(columns={\"judge_full\":\"judge\"})\n",
        "                 .sort_values([\"n_cases\",\"judge\"], ascending=[False, True])\n",
        "                 .head(5))\n",
        "\n",
        "    display(out[[\"judge\",\"n_cases\"]])\n",
        "    out[[\"judge\",\"n_cases\"]].to_csv(OUT_DIR / \"fl_supreme_top5_judges_by_cases.csv\", index=False)\n",
        "    print(\"Saved:\", OUT_DIR / \"fl_supreme_top5_judges_by_cases.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuB3YfBLFGmb",
        "outputId": "e168e666-d209-44c2-85ae-f048841f1816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No judge names found in FL Supreme Court entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# (Q3): Opinion-length distributions — courts of last resort\n",
        "# ------------------------------------------------------------\n",
        "# - Computes opinion length (words) from text if missing\n",
        "# - Identifies state courts of last resort (with NY/MA/WV exceptions)\n",
        "# - Summarizes by state: N, median, p25, p75, mean, min, max\n",
        "# - Saves two figures:\n",
        "#     A) heatmap of binned shares (appendix)\n",
        "#     B) ranked medians with IQR for top 15 states (main text)\n",
        "# - Writes summary tables to OUT_DIR\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# load cleaned cases\n",
        "READ_TSV = OUT_DIR / \"cases_clean.tsv\"\n",
        "READ_CSV = OUT_DIR / \"cases_clean.csv\"\n",
        "cases = pd.read_csv(READ_TSV, sep=\"\\t\", dtype=str, keep_default_na=False) if READ_TSV.exists() else pd.read_csv(READ_CSV, dtype=str, keep_default_na=False)\n",
        "\n",
        "# opinion length (words)\n",
        "cases[\"op_length_words\"] = pd.to_numeric(cases.get(\"op_length_words\", pd.Series(index=cases.index)), errors=\"coerce\")\n",
        "if cases[\"op_length_words\"].isna().any():\n",
        "    cases[\"op_length_words\"] = cases[\"op_length_words\"].fillna(\n",
        "        cases.get(\"text\", pd.Series(index=cases.index, dtype=\"object\")).fillna(\"\").map(lambda s: len(s.split()))\n",
        "    )\n",
        "cases[\"op_length_words\"] = pd.to_numeric(cases[\"op_length_words\"], errors=\"coerce\").fillna(0)\n",
        "\n",
        "# infer state from court_name if missing\n",
        "STATE_LIST = {\n",
        "    \"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\"Delaware\",\"Florida\",\"Georgia\",\n",
        "    \"Hawaii\",\"Idaho\",\"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\n",
        "    \"Massachusetts\",\"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\"Nevada\",\n",
        "    \"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\"Ohio\",\"Oklahoma\",\n",
        "    \"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\n",
        "    \"Vermont\",\"Virginia\",\"Washington\",\"West Virginia\",\"Wisconsin\",\"Wyoming\"\n",
        "}\n",
        "def infer_state_from_court(court_name: str) -> str|None:\n",
        "    if not isinstance(court_name, str): return None\n",
        "    for st in sorted(STATE_LIST, key=len, reverse=True):\n",
        "        if re.search(rf\"\\b{re.escape(st)}\\b\", court_name): return st\n",
        "    return None\n",
        "\n",
        "if \"state\" not in cases.columns:\n",
        "    cases[\"state\"] = cases[\"court_name\"].map(infer_state_from_court)\n",
        "else:\n",
        "    missing = cases[\"state\"].isna() | (cases[\"state\"].str.strip() == \"\")\n",
        "    cases.loc[missing, \"state\"] = cases.loc[missing, \"court_name\"].map(infer_state_from_court)\n",
        "\n",
        "# identify last-resort courts (default Supreme Court, plus exceptions)\n",
        "def is_last_resort(court_name: str, state_val: str) -> bool:\n",
        "    if not isinstance(court_name, str): return False\n",
        "    s = court_name.lower().strip()\n",
        "    st = (state_val or \"\").lower().strip()\n",
        "    if st == \"new york\":        # Court of Appeals of New York\n",
        "        return (\"court of appeals\" in s) and (\"new york\" in s)\n",
        "    if st == \"massachusetts\":   # Supreme Judicial Court\n",
        "        return \"supreme judicial court\" in s\n",
        "    if st == \"west virginia\":   # Supreme Court of Appeals of West Virginia\n",
        "        return \"supreme court of appeals\" in s and \"west virginia\" in s\n",
        "    if \"supreme court\" in s and st and st in s: return True\n",
        "    if re.search(rf\"\\b{re.escape(st)}\\b.*\\bsupreme court\\b\", s): return True\n",
        "    if re.search(r\"\\bsupreme court of the state of\\b\", s) and st in s: return True\n",
        "    if \"court_level\" in cases.columns:\n",
        "        try:\n",
        "            cl = cases.loc[cases[\"court_name\"]==court_name, \"court_level\"].head(1).iloc[0]\n",
        "            if cl == \"state_supreme\": return True\n",
        "        except Exception: pass\n",
        "    return False\n",
        "\n",
        "mask_lr = cases.apply(lambda r: is_last_resort(r.get(\"court_name\",\"\"), r.get(\"state\",\"\")), axis=1)\n",
        "last_resort = cases[mask_lr].copy()\n",
        "last_resort = last_resort[(last_resort[\"op_length_words\"] > 0) & last_resort[\"state\"].notna()]\n",
        "\n",
        "# summaries by state\n",
        "def summarize(g):\n",
        "    x = g[\"op_length_words\"].astype(float)\n",
        "    return pd.Series({\n",
        "        \"n_cases\": int(x.count()),\n",
        "        \"median\": float(np.median(x)) if len(x) else np.nan,\n",
        "        \"p25\": float(np.percentile(x, 25)) if len(x) else np.nan,\n",
        "        \"p75\": float(np.percentile(x, 75)) if len(x) else np.nan,\n",
        "        \"mean\": float(np.mean(x)) if len(x) else np.nan,\n",
        "        \"min\": float(np.min(x)) if len(x) else np.nan,\n",
        "        \"max\": float(np.max(x)) if len(x) else np.nan,\n",
        "    })\n",
        "summary_by_state = (last_resort.groupby(\"state\", as_index=True)\n",
        "                    .apply(summarize)\n",
        "                    .sort_values([\"n_cases\",\"median\"], ascending=[False, False]))\n",
        "\n",
        "summary_by_state.to_csv(OUT_DIR / \"opinion_length_summary_by_last_resort_state.csv\")\n",
        "display(summary_by_state.head(10))\n",
        "\n",
        "# binned shares (appendix heatmap)\n",
        "bins = [0, 1000, 2500, 5000, 10000, 20000, np.inf]\n",
        "labels = [\"0–1k\",\"1k–2.5k\",\"2.5k–5k\",\"5k–10k\",\"10k–20k\",\"20k+\"]\n",
        "tmp = last_resort.copy()\n",
        "tmp[\"len_bin\"] = pd.cut(tmp[\"op_length_words\"], bins=bins, labels=labels, right=False)\n",
        "bin_counts = (tmp.groupby([\"state\",\"len_bin\"]).size()\n",
        "                .reset_index(name=\"n\")\n",
        "                .pivot(index=\"state\", columns=\"len_bin\", values=\"n\")\n",
        "                .fillna(0).astype(int))\n",
        "bin_share = bin_counts.div(bin_counts.sum(axis=1), axis=0).fillna(0).round(3)\n",
        "bin_counts.to_csv(OUT_DIR / \"opinion_length_bins_by_last_resort_state_counts.csv\")\n",
        "bin_share.to_csv(OUT_DIR / \"opinion_length_bins_by_last_resort_state_shares.csv\")\n",
        "\n",
        "# figures\n",
        "FIG_DIR = (OUT_DIR / \"figures\"); FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# A) heatmap (appendix) — show states with at least MIN_CASES to avoid noise\n",
        "MIN_CASES = 5\n",
        "states_heat = summary_by_state.index[summary_by_state[\"n_cases\"] >= MIN_CASES].tolist()\n",
        "heat = bin_share.loc[states_heat, labels] if len(states_heat) else bin_share[labels]\n",
        "\n",
        "fig_h, ax_h = plt.subplots(figsize=(10, max(4, 0.35*len(states_heat))))\n",
        "im = ax_h.imshow(heat.values, aspect='auto')\n",
        "ax_h.set_yticks(range(heat.shape[0]))\n",
        "ax_h.set_yticklabels(list(heat.index))\n",
        "ax_h.set_xticks(range(len(labels)))\n",
        "ax_h.set_xticklabels(labels)\n",
        "ax_h.set_xlabel(\"Opinion length (word bins)\")\n",
        "ax_h.set_title(\"Opinion-length distribution shares — courts of last resort (all states)\")\n",
        "fig_h.colorbar(im, ax=ax_h, label=\"Share\")\n",
        "fig_h.tight_layout()\n",
        "heat_path = FIG_DIR / \"opinion_length_heatmap_last_resort_states.png\"\n",
        "fig_h.savefig(heat_path, dpi=300, bbox_inches=\"tight\")\n",
        "plt.close(fig_h)\n",
        "print(\"Saved heatmap:\", heat_path)\n",
        "\n",
        "# B) ranked medians with IQR (main text) — top 15 by N\n",
        "TOP_N = 15\n",
        "top = summary_by_state.head(TOP_N).copy()\n",
        "order = top.sort_values(\"median\", ascending=True).index.tolist()\n",
        "\n",
        "fig_d, ax_d = plt.subplots(figsize=(10, 0.5*len(order)))\n",
        "for i, st in enumerate(order, start=1):\n",
        "    p25, p50, p75 = top.loc[st, \"p25\"], top.loc[st, \"median\"], top.loc[st, \"p75\"]\n",
        "    ax_d.hlines(y=i, xmin=p25, xmax=p75)     # IQR line\n",
        "    ax_d.plot(p50, i, marker='o')            # median point\n",
        "ax_d.set_yticks(range(1, len(order)+1))\n",
        "ax_d.set_yticklabels(order)\n",
        "ax_d.set_xlabel(\"Opinion length (words)\")\n",
        "ax_d.set_title(f\"Opinion-length medians with IQR — top {TOP_N} states by cases\")\n",
        "fig_d.tight_layout()\n",
        "rank_path = FIG_DIR / \"opinion_length_ranked_median_iqr_top15.png\"\n",
        "fig_d.savefig(rank_path, dpi=300, bbox_inches=\"tight\")\n",
        "plt.close(fig_d)\n",
        "print(\"Saved ranked plot:\", rank_path)\n"
      ],
      "metadata": {
        "id": "5QUq_-Y2FPAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}