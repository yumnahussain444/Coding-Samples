{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-JIWg-zt44H"
      },
      "outputs": [],
      "source": [
        "#**********************************************************************\n",
        "#AUTHOR:     Yumna Hussain, yumnahussain444@gmail.com\n",
        "#DATE:       12/31/2025\n",
        "#*********************************************************************\n",
        "\n",
        "# ====== installing required packages and setting paths  ======\n",
        "\n",
        "!pip -q install beautifulsoup4 lxml nltk tqdm\n",
        "\n",
        "import re, time, json, gzip\n",
        "from io import StringIO\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from lxml import etree\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "CIK_LIST_PATH  = \"/content/drive/MyDrive/SEC_10Q/CIK_list.txt\"\n",
        "MASTER_IDX_PATH = \"/content/drive/MyDrive/SEC_10Q/form.idx\"\n",
        "USER_AGENT = \"Yumna Hussain (yumnahussain444@gmail.com)\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== reading the cik file and parsing the idx file  ======\n",
        "# ====== Read the CIK list and parse the SEC master index file ======\n",
        "\n",
        "def read_cik_list(path):\n",
        "    # This function extracts CIK numbers from a text file, normalizes them, and removes duplicates.\n",
        "    txt = open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
        "\n",
        "    # This finds any numeric token (up to 10 digits), which covers how CIKs appear in most lists.\n",
        "    ciks = re.findall(r\"\\b\\d{1,10}\\b\", txt)\n",
        "\n",
        "    # This keeps the first occurrence of each CIK and preserves the original order.\n",
        "    seen, out = set(), []\n",
        "    for c in ciks:\n",
        "        # Converting to int and back removes leading zeros so the CIK format is consistent.\n",
        "        c = str(int(c))\n",
        "        if c not in seen:\n",
        "            seen.add(c)\n",
        "            out.append(c)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _read_lines(path):\n",
        "    # This reads a text file into a list of lines, supporting both plain text and gzip-compressed files.\n",
        "    if path.lower().endswith(\".gz\"):\n",
        "        with gzip.open(path, \"rt\", encoding=\"latin1\", errors=\"ignore\") as f:\n",
        "            return f.read().splitlines()\n",
        "\n",
        "    with open(path, \"r\", encoding=\"latin1\", errors=\"ignore\") as f:\n",
        "        return f.read().splitlines()\n",
        "\n",
        "\n",
        "def load_master_idx_fixedwidth(path):\n",
        "    # This function parses the SEC master index format that is laid out like fixed-width text.\n",
        "    lines = _read_lines(path)\n",
        "\n",
        "    # Locate the header line so we know where the filings table begins.\n",
        "    header_i = None\n",
        "    for i, ln in enumerate(lines):\n",
        "        s = ln.replace(\"\\ufeff\", \"\").rstrip()\n",
        "        if (\"Form Type\" in s) and (\"Company Name\" in s) and (\"CIK\" in s) and (\"Date Filed\" in s) and (\"File Name\" in s):\n",
        "            header_i = i\n",
        "            break\n",
        "\n",
        "    if header_i is None:\n",
        "        raise ValueError(\"Could not find the master.idx table header line.\")\n",
        "\n",
        "    # The filings table usually starts after the dashed separator line that follows the header.\n",
        "    start_i = None\n",
        "    for j in range(header_i + 1, min(header_i + 10, len(lines))):\n",
        "        if re.fullmatch(r\"-{10,}\\s*\", lines[j].strip()):\n",
        "            start_i = j + 1\n",
        "            break\n",
        "\n",
        "    # If the separator line is missing, begin immediately after the header.\n",
        "    if start_i is None:\n",
        "        start_i = header_i + 1\n",
        "\n",
        "    # Parse each row into five fields: form type, company name, CIK, filing date, and file path.\n",
        "    row_re = re.compile(\n",
        "        r\"^(\\S+)\\s+(.*?)\\s+(\\d{1,10})\\s+(\\d{4}-\\d{2}-\\d{2})\\s+(edgar/data/\\d+/\\S+?\\.txt)\\s*$\"\n",
        "    )\n",
        "\n",
        "    rows = []\n",
        "    for ln in lines[start_i:]:\n",
        "        if not ln.strip():\n",
        "            continue\n",
        "\n",
        "        m = row_re.match(ln.rstrip())\n",
        "        if not m:\n",
        "            continue\n",
        "\n",
        "        form_type, company, cik, date_filed, filename = m.groups()\n",
        "        rows.append([cik.strip(), company.strip(), form_type.strip(), date_filed.strip(), filename.strip()])\n",
        "\n",
        "    if not rows:\n",
        "        raise ValueError(\"No filings rows were parsed from master.idx.\")\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\"CIK\", \"Company Name\", \"Form Type\", \"Date Filed\", \"File Name\"])\n",
        "    return df\n",
        "\n",
        "\n",
        "cik_list = read_cik_list(CIK_LIST_PATH)\n",
        "idx = load_master_idx_fixedwidth(MASTER_IDX_PATH)\n",
        "\n",
        "print(\"Loaded CIK count:\", len(cik_list))\n",
        "print(\"Parsed index row count:\", len(idx))\n",
        "\n",
        "idx.head()\n"
      ],
      "metadata": {
        "id": "wGY-uBHAz7zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== filter to each CIKs(latest) 10Q in Q2 2020  ======\n",
        "sub = idx[idx[\"CIK\"].isin(cik_list) & idx[\"Form Type\"].str.startswith(\"10-Q\", na=False)].copy()\n",
        "\n",
        "# prefer 10-Q over 10-Q/A, then most recent Date Filed\n",
        "sub[\"is_amendment\"] = sub[\"Form Type\"].str.contains(\"/A\", na=False).astype(int)\n",
        "sub = sub.sort_values([\"CIK\", \"is_amendment\", \"Date Filed\"], ascending=[True, True, False])\n",
        "sub = sub.groupby(\"CIK\", as_index=False).head(1)\n",
        "\n",
        "print(\"Matched 10-Q filings:\", len(sub), \"out of\", len(cik_list))\n",
        "sub.head()\n"
      ],
      "metadata": {
        "id": "psP5XqI50HFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== SEC downloader and word/sentence counts  ======\n",
        "session = requests.Session()\n",
        "session.headers.update({\n",
        "    \"User-Agent\": USER_AGENT,\n",
        "    \"Accept-Encoding\": \"gzip, deflate\",\n",
        "    \"Host\": \"www.sec.gov\",\n",
        "})\n",
        "\n",
        "def get_url_text(url, max_tries=6):\n",
        "    # This fetches a URL and retries on transient errors such as rate-limiting and server hiccups.\n",
        "    for k in range(max_tries):\n",
        "        r = session.get(url, timeout=60)\n",
        "\n",
        "        # A successful response is returned as text.\n",
        "        if r.status_code == 200:\n",
        "            r.encoding = r.encoding or \"utf-8\"\n",
        "            return r.text\n",
        "\n",
        "        # Indication of temporary blocking or overload\n",
        "        if r.status_code in (403, 429, 500, 502, 503, 504):\n",
        "            time.sleep(1.5 * (k + 1))\n",
        "            continue\n",
        "\n",
        "        # Flaging failure\n",
        "        r.raise_for_status()\n",
        "\n",
        "    raise RuntimeError(f\"Failed after retries: {url} (last status {r.status_code})\")\n",
        "\n",
        "_word_re = re.compile(r\"\\b\\w+\\b\", flags=re.UNICODE)\n",
        "\n",
        "def filing_text_metrics(raw_filing_text):\n",
        "    # This converts the filing HTML into plain text, normalizes spacing, and computes text-length metrics.\n",
        "    soup = BeautifulSoup(raw_filing_text, \"lxml\")\n",
        "    txt = soup.get_text(\" \", strip=True)\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "\n",
        "    word_count = len(_word_re.findall(txt))\n",
        "    sentence_count = len(sent_tokenize(txt)) if txt else 0\n",
        "\n",
        "    return word_count, sentence_count\n",
        "\n"
      ],
      "metadata": {
        "id": "IcA2xJV_1LI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_cik_and_accession(filename):\n",
        "    # This extracts the CIK directory and the accession number (with dashes removed) from an EDGAR filing path.\n",
        "    # Example: edgar/data/1652044/0001652044-20-000021.txt -> (\"1652044\", \"000165204420000021\")\n",
        "    parts = filename.strip(\"/\").split(\"/\")\n",
        "    if len(parts) < 4 or parts[-3].lower() != \"data\":\n",
        "        return (None, None)\n",
        "\n",
        "    cik = parts[-2].strip()\n",
        "    accession = parts[-1].replace(\".txt\", \"\").strip().replace(\"-\", \"\")\n",
        "    return (cik, accession)\n",
        "\n",
        "\n",
        "def get_index_json_url(filename):\n",
        "    # This builds the URL to the directory listing for a filing, which is used to discover the instance XML file.\n",
        "    cik, acc = parse_cik_and_accession(filename)\n",
        "    if not cik or not acc:\n",
        "        return None\n",
        "    return f\"https://www.sec.gov/Archives/edgar/data/{cik}/{acc}/index.json\"\n",
        "\n",
        "\n",
        "def download_best_instance_xml(filename):\n",
        "    # This selects the most likely instance XML from index.json by excluding common non-instance XMLs\n",
        "    # and taking the largest remaining XML file.\n",
        "    idx_url = get_index_json_url(filename)\n",
        "    if not idx_url:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        j = json.loads(get_url_text(idx_url))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    items = j.get(\"directory\", {}).get(\"item\", [])\n",
        "    if not items:\n",
        "        return None\n",
        "\n",
        "    # These names are typically linkbases, schemas, stylesheets, or summaries rather than the instance document.\n",
        "    excluded_tokens = (\"cal\", \"def\", \"lab\", \"pre\", \"xsd\", \"xsl\", \"ref\", \"schema\", \"filingsummary\")\n",
        "\n",
        "    candidates = []\n",
        "    for it in items:\n",
        "        name = (it.get(\"name\") or \"\")\n",
        "        lname = name.lower()\n",
        "\n",
        "        if not lname.endswith(\".xml\"):\n",
        "            continue\n",
        "        if any(tok in lname for tok in excluded_tokens):\n",
        "            continue\n",
        "\n",
        "        size = int(it.get(\"size\") or 0)\n",
        "        candidates.append((size, name))\n",
        "\n",
        "    if not candidates:\n",
        "        return None\n",
        "\n",
        "    candidates.sort(reverse=True)  # largest first\n",
        "    best_name = candidates[0][1]\n",
        "\n",
        "    cik, acc = parse_cik_and_accession(filename)\n",
        "    xml_url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{acc}/{best_name}\"\n",
        "\n",
        "    try:\n",
        "        return get_url_text(xml_url)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_eps_two_latest_quarters(xbrl_xml_text):\n",
        "    # This extracts quarterly (roughly 80â€“100 day) diluted EPS values and returns the two most recent quarters.\n",
        "    parser = etree.XMLParser(recover=True, huge_tree=True)\n",
        "    try:\n",
        "        root = etree.fromstring(xbrl_xml_text.encode(\"utf-8\", errors=\"ignore\"), parser=parser)\n",
        "    except Exception:\n",
        "        return (None, None)\n",
        "\n",
        "    # This maps each context id to its (startDate, endDate) so facts can be filtered to quarterly durations.\n",
        "    ctx_period = {}\n",
        "    for ctx in root.findall(\".//{*}context\"):\n",
        "        cid = ctx.get(\"id\")\n",
        "        if not cid:\n",
        "            continue\n",
        "        start = ctx.findtext(\".//{*}period/{*}startDate\")\n",
        "        end = ctx.findtext(\".//{*}period/{*}endDate\")\n",
        "        if start and end:\n",
        "            ctx_period[cid] = (start.strip(), end.strip())\n",
        "\n",
        "    facts = []\n",
        "    for el in root.iter():\n",
        "        try:\n",
        "            if etree.QName(el).localname != \"EarningsPerShareDiluted\":\n",
        "                continue\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        ctxref = el.get(\"contextRef\")\n",
        "        val_txt = (el.text or \"\").strip()\n",
        "\n",
        "        if not ctxref or not val_txt or ctxref not in ctx_period:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            val = float(val_txt)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        start, end = ctx_period[ctxref]\n",
        "        try:\n",
        "            s = pd.to_datetime(start)\n",
        "            e = pd.to_datetime(end)\n",
        "            days = (e - s).days\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if 80 <= days <= 100:\n",
        "            facts.append((e, val))\n",
        "\n",
        "    if not facts:\n",
        "        return (None, None)\n",
        "\n",
        "    facts.sort(key=lambda x: x[0])  # by quarter end date\n",
        "\n",
        "    # This keeps one EPS value per quarter-end date and returns the two most recent quarters.\n",
        "    seen_dates = set()\n",
        "    latest = []\n",
        "    for end, val in reversed(facts):\n",
        "        key = end.date().isoformat()\n",
        "        if key in seen_dates:\n",
        "            continue\n",
        "        seen_dates.add(key)\n",
        "        latest.append(val)\n",
        "        if len(latest) == 2:\n",
        "            break\n",
        "\n",
        "    if len(latest) == 1:\n",
        "        return (latest[0], None)\n",
        "    return (latest[0], latest[1])\n",
        "\n",
        "\n",
        "def extract_two_quarterly_eps(filename):\n",
        "    # This uses only the index.json approach and returns (current_quarter_eps, previous_quarter_eps).\n",
        "    xml = download_best_instance_xml(filename)\n",
        "    if not xml:\n",
        "        return (None, None)\n",
        "    return parse_eps_two_latest_quarters(xml)\n",
        "\n"
      ],
      "metadata": {
        "id": "pTWpllQJ4Ey7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_master_columns(df):\n",
        "    # This standardizes column names so downstream code can rely on one consistent schema.\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    # This mapping is keyed by a normalized version of the column name (lowercase, no spaces, no trailing periods).\n",
        "    canonical = {\n",
        "        \"cik\": \"CIK\",\n",
        "        \"companyname\": \"Company Name\",\n",
        "        \"formtype\": \"Form Type\",\n",
        "        \"datefiled\": \"Date Filed\",\n",
        "        \"filename\": \"Filename\",\n",
        "        \"file name\": \"Filename\",  # included as a direct special-case label\n",
        "    }\n",
        "\n",
        "    rename_map = {}\n",
        "    for c in df.columns:\n",
        "        # This normalization makes \"File Name\", \"file name\", and \"File   Name\" comparable.\n",
        "        cleaned = c.strip().lower()\n",
        "        key_no_spaces = cleaned.replace(\" \", \"\")\n",
        "        key_no_dot = key_no_spaces.rstrip(\".\")\n",
        "\n",
        "        # This prefers the robust \"no spaces + no trailing dot\" key, but also supports the literal \"file name\".\n",
        "        if cleaned in canonical:\n",
        "            rename_map[c] = canonical[cleaned]\n",
        "        elif key_no_dot in canonical:\n",
        "            rename_map[c] = canonical[key_no_dot]\n",
        "\n",
        "    return df.rename(columns=rename_map)\n",
        "idx = normalize_master_columns(idx)\n",
        "sub = normalize_master_columns(sub)\n",
        "\n",
        "print(\"idx columns:\", idx.columns.tolist())\n",
        "print(\"sub columns:\", sub.columns.tolist())\n",
        "\n"
      ],
      "metadata": {
        "id": "71r98xy44S1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for _, r in tqdm(sub.iterrows(), total=len(sub)):\n",
        "    cik = r[\"CIK\"]\n",
        "    filing_date = r[\"Date Filed\"]\n",
        "    filename = r[\"Filename\"]\n",
        "    filing_url = \"https://www.sec.gov/Archives/\" + filename.lstrip(\"/\")\n",
        "\n",
        "    raw_txt = get_url_text(filing_url)\n",
        "    word_count, sentence_count = filing_text_metrics(raw_txt)\n",
        "    eps_cur, eps_prev = extract_two_quarterly_eps(filename, raw_txt)\n",
        "\n",
        "    rows.append({\n",
        "        \"CIK\": cik,\n",
        "        \"filing_date\": filing_date,\n",
        "        \"word_count\": word_count,\n",
        "        \"sentence_count\": sentence_count,\n",
        "        \"eps_current_q\": eps_cur,\n",
        "        \"eps_previous_q\": eps_prev,\n",
        "        \"filing_url\": filing_url\n",
        "    })\n",
        "\n",
        "    time.sleep(0.35)\n",
        "\n",
        "out = pd.DataFrame(rows).sort_values(\"CIK\").reset_index(drop=True)\n",
        "out\n"
      ],
      "metadata": {
        "id": "9hJCWzzs4Hl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_PATH = \"/content/drive/MyDrive/edgar_q2_2020_10q_metrics.csv\"\n",
        "out.to_csv(OUT_PATH, index=False)\n",
        "print(\"Saved to:\", OUT_PATH)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(OUT_PATH)\n"
      ],
      "metadata": {
        "id": "DiqQnwsz-ScP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}